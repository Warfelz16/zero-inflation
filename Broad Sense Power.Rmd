---
title: "R Notebook"
output: html_notebook
---

**Power analysis for experimental design**

Learning how to run simulations can be helpful because it allows you to generate a lot of data based on the characteristics of a small amount of data. This might sound fishy, but it depends on the context and how the simulated data is used.  For instance, making up data is completely appropriate if you want to run a power analysis, or test the precision/accuracy of a statistic.  


We will simulate data in other contexts in this class, and so it is useful to spend some time to understand the logic behind the code that generates the simulations.

**Introduction to power analysis**

Power analysis it about minimizing type II error, or in other words minimizing the risk that you will not detect an effect that is
really there with your data and analysis.  Risking making a Type II error is often considered to be more acceptable than making a 
Type I error (Detecting an effect that is not really there), which tend to get more focus due in part to the “P<0.05 is significant” mentality. However, the consequences of making a type I or II error can be equally severe depending on context.  For instance, having a low type I error rate is really important if you want to determine if a new COVID-19 vaccine or therapy actually works. But in fields like conservation science ecology and evolution, type II errors may be more consequential. For example, if we are studying the effects of environmental change , then Poor power would mean we are unlikely to detect environmental change in an experiment, or to detect the effects of an
oil spill on a rare seabird colony.

So you’d want enough power to detect an effect, should it be there.

Power analysis can be especially useful for minimizing type II errors, when used in the broader sense of the term. Broad-sense power analysis is used to determine how well your (or someone elses data) and a statistical model work together to measure an effect (in other words determining if you are quantifying the right effect given your model).

Regardless of whether you run a more traditiona power analysis (see text by Quinn and Keough book on Canvas) or a broad sense power analysis (covere in more depth in Bolker 2008 - Ecological Models and Data in R)--A power analysis is a really helpful tool when designing experiments, or field surveys.

**Running a Power analysis for experimental design**
*This example is borrowed and modified from a blueecology blog on R-bloggers.*

Let’s say you want to know whether there are more fish inside a marine reserve (that have no fishing) than outside the reserve. You are going
to do a number of standardized transects inside and outside the reserven and count the numbers of fish.

Your fish species of interest are a very abundant sweetlips and a rather rare humphead wrasse. You want to know your chances of detecting a 2x difference in the abundance of each fish species inside the reserves where fishing is not permited versus outside the reserves where both the species can be fished.

We can use a power analysis to address this question.  The power analysis is acccomplished by simulating ‘fake’ data for the surveys where we can impose a doubling of abundance. We can then fit our proposed statistical model  to the fake data (i.e., run a statistical analysis), then decide whether or not we can detect a difference in the abundances that are ‘significant’ based on some arbitrarily determined critical value (e.g. p<0.05). Then we can repeat that "fake" or simulated experiment a 1000 times (note that each simulated exeperiment is subject to sampling error within and without the reserve zones) and count up the % of times we said there was a difference. That calculated % is wht we call the power of the test.

So the things that affect the power of a test are the same as those things we have been learning affect the precision of a sample (i.e., sample size and sampling error). Therefore, we need to know the expected sample size of surveys, the expected (mean) abundance values (inside and outside the reserves) and an estimate of the variance or spread in the abundance samples. To get estimates of means and variances you could draw on earlier literature to make estimated guesses (assuming they provide unbiased estimates of the true population parameters). But, the sample size is not a characterisitic of true population and so trying a range of sample sizes could be part of your power analysis.

Let’s assume there are normally 10 sweetlips per transect and 1 humphead wrasse per transect.

As the data are counts we’ll assume they are Poisson distributed (dont worry if you are not familiar with Poisson distributions - just go with this for now becuase the shape of the distribution is not important for this but we will cover Poisson and other distributions in great detail in a few weeks). The Poisson distribution assumes mean = variance (which simplifies this example a little), so the variance of sweetlips across transects is 10 and wrasse is 1.

**Simulating the data**

We are going to use quite a few packages that can be quite useful for simulations (you may need to install some of these):
```{r}
library(purrr)
library(ggplot2)
library(broom)
library(dplyr)
library(tidyr)
```


`purrr` for creating 1000s of randomised datasets
`ggplot2` for plots, 
`broom` is for sweeping up (literatlly for cleaning) the 1000s of models youll fit,
`dplyr` and `tidyr` are for data wrangling--and as you now know :)

Now we will create a function that simulates data and fits a model. This may look overwhelming, but don’t worry!  
We will start off by writing a simple function to calculate a statistic of interest - in this case the mean of a column of data. We will a simple matrix for our data set to illustrate the format as illustrated below -- We’ll call our function “mean_calculator”. 


```{r}
x_data=matrix(1:100,nrow=25,ncol=4)
mean_calculator <- function(x,i){mean(x[,i])}
```
So now there is a function in the R environment called mean_calculator (be careful wiht naming because if you just named this function `mean` it would overwrite the built in function that calculates the mean).  This function works just like a built in function in R.  There are arguments that must be specified to operate the function.  These are dictated in the parentheses after the `function` operator.  For instance here, `x` refers to the data and `i` serves as a counter to identify which column. Using your function is illustrated below.

```{r}
mean_calculator(x_data,1)
mean_calculator(x_data,3)
mean_calculator(x_data,c(1,4))
```
*Make sure you understand the logic of writing a function - this is a super useful skill!!!*

Okay, so now we will do something more sophisticated but conceptually identical to the function we wrote above. 

'We are going to write a function to create a function that simulates some data from two groups (reserve or not reserve) for n transects, and then fits a GLM (generalized linear model - details not important well learn about them in in a few weeks) and finally it spits out a p-value for whether there was a significant difference in the simulated data.

```{r}
simulator <- function(n, x1, x2){
  x <- rep(c(x1, x2), each = n/2) #this creates a column labeling the two factors
  y <- rpois(n, lambda = x) #this takes a random sample from with sample size n from a poisson dist.
  m1 <- glm(y ~ x, family = "poisson") %>% tidy() #this runs a GLM and tidy's the output
  m1
}

```

Now we can use the `simulator` function your wrote to simulate counting wrasse on 100 transects (50 inside and 50 outside the reserve).  You can set a seed for the random number generator to make sure you get the same answer everytime.  The mean inside is 1 (arbitrary here) and outside is 2 (a 2x difference)

```{r}
set.seed(2001) #just do this to get the same result as me
simulator(100, 1, 2)
```
So you should have gotten a table with mean estimated difference (on log scale), standard errors and p-values.

*Narrow-sense power analysis*

Now we can use the purrr library to do this 1000 times except we will make n=20:

```{r}
?map
mout <- map(1:1000, ~simulator(20, 1, 2))
```


THis generates 1000 lists of output similar to what we saw for the single run, but 1000 lists is very messy so you need to wrangle these output data a little using dply. 
 
```{r}
mout2 <- mout %>%
  bind_rows(.id = "rep") %>%
  filter(term != "(Intercept)") %>%
  mutate(Signif = p.value < 0.05,
         rep = as.numeric(rep))
head(data.frame(mout2))
```


Now you should have a dataframe of the 1000 simulations, indicating whether p for
the difference between reserve vs unreserved was <0.05 (column ‘Signif’).

To get the power, we just sum Signif and divide by the 1000 trials:

```{r}

```

So there is an approx ~40% chance thart you would detect a 2x difference in wrasse abundance with
20 transects. This is the 2-sided probability, arguably for this question
we could also use a one-sided test.

**Challenge**
**1.** Try it again for the sweetlips (expected abundance doubling from 10 to 20). You’ll see you get much more power        with this more abundant species (almost 100%).

```{r}
mlip <- map(1:1000, ~simulator(20, 10, 20))
mlip2 <- mlip %>%
  bind_rows(.id = "rep") %>%
  filter(term != "(Intercept)") %>%
  mutate(Signif = p.value < 0.05,
         rep = as.numeric(rep))
head(data.frame(mlip2))
sum(mlip2$Signif)/1000
```
**2.** Try this with different sample sizes for both species to get an idea of how much effort you need to invest in doing transects in order to see a difference (if the difference is really there of course).


**Broad-sense power analysis**

How good does our statsitical model estimate the effect??? OR, How close does our approach get us to the 2x difference? We can answer that by looking at the estimates from the GLM:

```{r}
ggplot(mout2, aes(x = exp(estimate))) +
  geom_density(fill = "tomato") +
  theme_bw() +
  geom_vline(xintercept = 2) +
  xlab("Estimated times difference")
```


This distribution shows the expected outcomes we’d estimate over 1000 repeats of the surveys. So the solid vertical line is the ‘real’ difference. Note the long tail to the left of drastic overestimates. It
is common with small sample sizes that we might overestimate the true
effect size. More on this later.

Now for reasons you will learn about later you may have notice that we took the exponent of the estimate (estimated mean difference) in the ggplot function, this is because the Poisson GLM has a log link, so the estimate is on the log scale. Taking its exponent means it is now interpreted as a times difference (as per the x-axis label).


**Bias in significant estimates**

It is reasonably well known that over-use of p-values can contribute to publication bias, where scientists tend to publish papers about significant and possibly overestimated effect sizes, but never publish the non-significant results. This bias can be particularly bad with small sample sizes, because there is a reasonable chance we’ll see a big difference and therefore, make a big deal about it.

We can look at this phenomena in our simulations. First, let’s take the mean of our estimated effect sizes for those trials that were significant and those that were not:

```{r}
signif_mean <- mean(exp(filter(mout2, Signif)$estimate))
nonsignif_mean <- mean(exp(filter(mout2, !Signif)$estimate))
all_mean <- mean(exp(mout2$estimate))
c(all_mean, signif_mean, nonsignif_mean)
```


So average effect size for the significant trials is >3x (remember the real difference is 2x). If we take the average across all trials it is closer to the truth (2.3x).

Clearly if we only publish the significant results, over many studies this will add up to a much bigger difference than is really there. This can be a problem in some fields. Publication bias may not be particularly problematic in studies of affects of marine reserves, because typically there are multiple research questions, so the researchers will publish anyway.

Now we can look at this as a plot using the same distribution as above, but with different colours for significant versus non-significant.

```{r}
ggplot(mout2, aes(x = exp(estimate), fill = Signif)) +
  geom_density(alpha = 0.5) +
  theme_bw() +
  geom_vline(xintercept = 2) +
  xlab("Estimated times difference") +
  xlim(0,5)
```


You can clearly see the significant trials almost always overestimate the true difference (vertical line).

So, make sure you report on non-significant results. And try to aim for larger sample sizes.